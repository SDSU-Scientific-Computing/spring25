{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Introduction to Vectorization\n",
    "\n",
    "Last time:\n",
    "\n",
    "- Introduction to architectures\n",
    "- Memory\n",
    "\n",
    "Today:\n",
    "1. [Single-thread performance trends](#single-thread-performance-trends)  \n",
    "2. [Introduction to Vectorization](#introduction-to-vectorization)\n",
    "\n",
    "## 1. Single-thread performance trends\n",
    "\n",
    "Single-thread performance has increased significantly\n",
    "since ~2004 when clock frequency stagnated (Moore's law stopped)?\n",
    "\n",
    "![42 years of microprocessor data](https://www.karlrupp.net/wp-content/uploads/2018/02/42-years-processor-trend.png)\n",
    "\n",
    "Source: [Microprocessor Trend Data](https://github.com/karlrupp/microprocessor-trend-data).\n",
    "\n",
    "How is this possible?\n",
    "\n",
    "This is a result of doing more per clock cycle.\n",
    "\n",
    "![Flops per clock cycle](https://www.karlrupp.net/wp-content/uploads/2013/06/flops-per-cycle-sp.png)\n",
    "\n",
    "\n",
    "### Further resources\n",
    "\n",
    "* [Intel Intrinsics Guide](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#)\n",
    "* [Agner Fog's website](https://www.agner.org/optimize/)\n",
    "\n",
    "## 2. Introduction to Vectorization\n",
    "\n",
    "- History: Starting with the Cray 1 supercomputer, vector systems had dominated scientific and technical computing for a long time until powerful RISC-based massively parallel machines became available.\n",
    "- Today, they are part of a niche market for high-end technical computing with extreme demands on memory bandwidth and time to solution.\n",
    "- By design, vector processors show a much better ratio of real application performance to peak performance than standard microprocessors for suitable \"_vectorizable_\" code.\n",
    "- They follow the **SIMD (Single Instruction Multiple Data)** paradigm which demands that a single machine instruction is automatically applied to a — presumably large — number of arguments of the same type, i.e., a _vector_.\n",
    "- Most modern cache-based microprocessors have adopted some of those ideas in the form of _SIMD instruction set extensions_.\n",
    "- For vector processors, machine instructions operate on _vector registers_ which can hold a number of arguments, usually between 64 and 256 (double precision).\n",
    "- The width of a vector register is called the vector length, usually denoted by $L_v$.\n",
    "\n",
    "### Example:\n",
    "\n",
    "For getting reasonable performance out of a vector CPU, SIMD-type instructions must be employed.\n",
    "\n",
    "As a simple example we consider the addition, of two arrays as a Fortran code snippet::\n",
    "\n",
    "```Fortran\n",
    "A(1:N)=B(1:N)+C(1:N)\n",
    "```\n",
    "\n",
    "On a cache-based microprocessor this would result in a (possibly software-pipelined) loop over the elements of `A`, `B`, and `C`. For each index, two _loads_ (one for `B` and one for `C`), one _addition_ and one _store_ (in the result vector `A`) operation would have to be executed, together with the required integer and branch logic to perform the loop.\n",
    "\n",
    "A vector CPU can issue a single instruction for a whole array if it is shorter than the vector register length $L_v$:\n",
    "\n",
    "```\n",
    "vload V1(1:N) = B(1:N)\n",
    "vload V2(1:N) = C(1:N)\n",
    "vadd V3(1:N) = V1(1:N) + V2(1:N)\n",
    "vstore A(1:N) = V3(1:N)\n",
    "```\n",
    "\n",
    "Here, `V1`, `V2`, and `V3` denote _vector registers_.\n",
    "\n",
    "The distribution of vector indices across the pipeline tracks is automatic. If the array length is larger than the vector length, the loop must be _stripmined_, i.e., the original arrays are traversed in chunks of the vector length:\n",
    "\n",
    "```\n",
    "do S = 1,N,Lv !  start, stop [,step]  \n",
    "    E = min(N,S+Lv -1)\n",
    "    L = E-S+1\n",
    "    vload V1(1:L) = B(S:E)\n",
    "    vload V2(1:L) = C(S:E)\n",
    "    vadd V3(1:L) = V1(1:L) + V2(1:L)\n",
    "    vstore A(S:E) = V3(1:L)\n",
    "enddo\n",
    "```\n",
    "\n",
    "This is done automatically by the compiler.\n",
    "\n",
    "An operation like vector addition does not have to wait until its argument vector registers are completely filled but can commence after some initial arguments are available. This feature is called _chaining_ and forms an essential requirement for\n",
    "different pipes (like `MULT` and `ADD`) to operate concurrently.\n",
    "\n",
    "- Writing a program so that the compiler can generate effective SIMD vector instructions is called **vectorization**.\n",
    "  * Sometimes this requires reformulation of code or inserting source code directives in order to help the compiler identify SIMD parallelism.\n",
    "- A necessary prerequisite for vectorization is the lack of true data dependencies across the iterations of a loop.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
