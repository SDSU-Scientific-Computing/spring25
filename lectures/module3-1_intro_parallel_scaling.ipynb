{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11) Introduction to Parallel Scaling\n",
    "\n",
    "Las- Intro to Multithreading\n",
    "\n",
    "Today:\n",
    "1. [Programs with more than one part](#programs-with-more-than-one-part)  \n",
    "2. [Scalability](#scalability)  \n",
    "  2.1 [Scalability metrics](#scalability-metrics)  \n",
    "  2.2 2.1 [Scalability laws](#scalability-laws)\n",
    "3. [Strong scaling](#strong-scaling)  \n",
    "4. [Efficiency](#efficiency)  \n",
    "\n",
    "\n",
    "## 1. Programs with more than one part\n",
    "\n",
    "So far, we've focused on simple programs with only one part, but real programs have several different parts, often with data dependencies.\n",
    "\n",
    "Some parts will be amenable to optimization and/or parallelism and others will not.\n",
    "\n",
    "![Diminishing returns](https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Optimizing-different-parts.svg/2880px-Optimizing-different-parts.svg.png)\n",
    "\n",
    "\n",
    "This principle is called [Amdahl's Law](https://en.wikipedia.org/wiki/Amdahl%27s_law), which is a formula that shows how much faster a task can be completed when more resources are added to the system.\n",
    "\n",
    "Suppose that a fraction $f$ of the total work is amenable to optimization. We run a problem size $n$ with parallelization (or parallelizable) factor $p$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function exec_time(f, p; n=10, latency=1)\n",
    "    # Suppose that a fraction f of the total work is amenable to optimization\n",
    "    # We run a problem size n with parallelization factor p\n",
    "    return latency + (1-f)*n .+ f*n./p\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a few fractions for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using DataFrames\n",
    "using Printf\n",
    "default(linewidth=4, legendfontsize=12)\n",
    "\n",
    "ps = exp10.(range(log10(1), log10(1000), length=50))\n",
    "\n",
    "plot(ps, [exec_time(.99, ps, latency=0), exec_time(1, ps, latency=0)],\n",
    "    xscale=:log10, yscale=:log10, labels=[\"f=0.99\" \"f=1\"],\n",
    "    title=\"A first look at strong scaling\", xlabel=\"p\", ylabel=\"time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scalability\n",
    "\n",
    "### 2.1 Scalability metrics\n",
    "\n",
    "In order to be able to define scalability we first have to identify the basic measurements on which derived performance metrics are built. \n",
    "\n",
    "In a simple model, the overall problem size (\"amount of work\") shall be $s + p = 1$, where $s$ is the **serial** (or sequential, nonparallelizable) part and $p$ is the perfectly parallelizable fraction. \n",
    "\n",
    "There can be many reasons for a nonvanishing serial part:\n",
    "\n",
    "- _Algorithmic limitations_. Operations that cannot be done in parallel because of, e.g., mutual dependencies, can only be performed one after another, or even in a certain order.\n",
    "- _Bottlenecks_. Shared resources are common in computer systems: Execution units in the core, shared paths to memory in multicore chips, I/O devices. Access to a shared resource serializes execution. Even if the algorithm itself could be performed completely in parallel, concurrency may be limited by bottlenecks.\n",
    "- _Startup overhead_. Starting a parallel program, regardless of the technical details, takes time. Of course, system designs try to minimize startup time, especially in massively parallel systems, but there is always a nonvanishing serial part. If a parallel application's overall runtime is too short, startup will have a strong impact. (This is also why usually the first elapsed time of the execution of a benchmark program is discarded when measuring performance).\n",
    "- _Communication_. Fully concurrent  communication between different parts of a parallel system cannot be taken for granted. If solving a problem in parallel requires communication, some serialization is usually unavoidable. Some communication processes limit scalability as they cannot be overlaped with each other or with calculation.\n",
    "    * Communication can be accounted for in scalability metrics in a more elaborate way than just adding a constant to the serial fraction.\n",
    "\n",
    "We assume a fixed problem (of a _fixed size_), which is to be solved by $N$ workers. We normalize the single-worker (serial) runtime\n",
    "\n",
    "$$\n",
    "T^s_\\textrm{fixed size} = s + p\n",
    "$$\n",
    "\n",
    "to $1$. \n",
    "\n",
    "Solving the same problem on $N$ workers will require a runtime of\n",
    "\n",
    "$$\n",
    "T^p_\\textrm{fixed size} = s + \\frac{p}{N}.\n",
    "$$\n",
    "\n",
    "This is called **strong scaling** because the amount of work stays constant no matter how many workers are used. Here the goal of parallelization is minimization of time to solution for a given problem.\n",
    "\n",
    "If time to solution is not the primary objective because larger problem sizes (for which available memory is the limiting factor) are of interest, it is appropriate to scale the problem size with some power of $N$ so that the total amount of work is $s + pN^{\\alpha}$ , where $\\alpha$ is a positive but otherwise free parameter. Here we use the implicit assumption that the serial fraction $s$ is a constant. \n",
    "\n",
    "We define the serial runtime for the scaled (_variably-sized_) problem as\n",
    "\n",
    "$$\n",
    "T^s_\\textrm{variable size} = s +pN^{\\alpha}.\n",
    "$$\n",
    "\n",
    "Consequently, the parallel runtime in this case is:\n",
    "\n",
    "$$\n",
    "T^p_\\textrm{variable size} = s +pN^{\\alpha -1}.\n",
    "$$\n",
    "\n",
    "The term **weak scaling** has been coined for this approach, although it is commonly used only for the special case $\\alpha = 1$.\n",
    "\n",
    "\n",
    "### 2.1 Scalability laws\n",
    "\n",
    "In a simple ansatz, _application speedup_ can be defined as the quotient of parallel and serial performance for fixed problem size. In the following we define \"performance\" as \"work over time\", unless otherwise noted.\n",
    "\n",
    "Serial performance for fixed problem size (work) $s + p$ is thus\n",
    "\n",
    "$$\n",
    "P^s_{\\textrm{fixed}} = \\frac{s + p}{T^s_{\\textrm{fixed}}}  = 1, \n",
    "$$\n",
    "\n",
    "as exptected. \n",
    "\n",
    "Parallel performance is in this case\n",
    "\n",
    "$$\n",
    "P^p_{\\textrm{fixed}} = \\frac{s + p}{T^p_{\\textrm{fixed}}}(N)  = \\frac{1}{s + \\frac{1-s}{N}}, \n",
    "$$\n",
    "\n",
    "and application speedup (as \"scalability\") is\n",
    "\n",
    "$$\n",
    "S_{\\textrm{fixed}} = \\frac{P^p_{\\textrm{fixed}}}{P^s_{\\textrm{fixed}}} = \\frac{1}{s + \\frac{1-s}{N}} , \\qquad \\leftarrow \\text{Amdahl's Law}.\n",
    "$$\n",
    "\n",
    "We have derived Amdahl's Law. \n",
    "\n",
    "This law limits the application speedup for (hypothetical) $N \\rightarrow \\infty$ to $1/s$. \n",
    "\n",
    "This law tries to answer the question:\n",
    "- [**Strong scaling**]: How much faster (in terms of runtime) does my application run when I put the same problem on $N$ CPUs (i.e., more resources)?\n",
    "\n",
    "As opposed to weak scaling where workload grows with CPU count:\n",
    "- [**Weak scaling**]: How much more work can my program do in a given amount of time when I put a larger problem on $N$ CPUs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Strong scaling\n",
    "\n",
    "### Fixed total problem size.\n",
    "\n",
    "Cost = `time * p`, with `p` parallelization factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function exec_cost(f, p; args...)\n",
    "    return exec_time(f, p; args...) .* p\n",
    "end\n",
    "\n",
    "plt = plot(ps, exec_cost(.99, ps), xscale=:log10, yscale=:log10, title = \"Strong scaling: cost Vs p\", xlabel = \"p\", ylabel = \"cost\", label = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above function, we have used Julia's \"[splat](https://docs.julialang.org/en/v1/base/base/#...)\" for a sequence of arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Efficiency\n",
    "\n",
    "Efficiency can be viewed from the perspective of the time, the resources being allocated, or the energy that is consumed by the hardware during the job runtime (energy efficiency). \n",
    "\n",
    "\n",
    "### Parallel efficiency\n",
    "We want to ask ourselves how effectively a given resource, i.e., CPU computational power, can be used in a parallel program.\n",
    "\n",
    "In the following, very simplified approach, we assume that the serial part of the program is executed on one single worker while all others have to wait. Hence, parallel efficiency can be defined as:\n",
    "\n",
    "$$\n",
    "\\varepsilon = \\frac{\\textrm{performance on } N \\textrm{ CPUs}}{N \\times \\textrm{performance on one CPU}} = \\frac{\\textrm{speedup}}{N} .\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "To simplify for now we consider:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\textrm{cost}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(ps, 1 ./ exec_cost(.99, ps, latency=1), xscale=:log10, title = \"Strong scaling: efficiency Vs p\", xlabel = \"p\", ylabel = \"efficiency\", ylims = [0, Inf], label = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a simplified version of speedup:\n",
    "\n",
    "$$\n",
    "S(p) = frac{T(1)}{T(p)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(ps, exec_time(.99, 1, latency=1) ./ exec_time(0.99, ps, latency=1), ylims = [0, 10], title = \"Strong scaling: speedup Vs p\", xlabel = \"p\", ylabel = \"speedup\", label = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.6",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
